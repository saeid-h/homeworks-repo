{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Your-Name?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run my code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    one_hot = numpy.zeros((len(y), num_class))\n",
    "    for i in range(len(y)):\n",
    "        one_hot[i, y[i]] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "# train_indices = rand_indices[0:4000]\n",
    "# valid_indices = rand_indices[4000:5000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_313 (Conv2D)          (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_314 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_137 (Activation)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_133 (MaxPoolin (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_315 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_138 (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_316 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_139 (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_134 (MaxPoolin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_317 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_140 (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_318 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_141 (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_135 (MaxPoolin (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_142 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_143 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 849,194\n",
      "Trainable params: 848,234\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Add, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "# A simple sequential model\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3),  padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(128, (3, 3),padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 1E-3 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 132s 105ms/step - loss: 1.5666 - acc: 0.4223 - val_loss: 1.6435 - val_acc: 0.4457\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 1.2100 - acc: 0.5680 - val_loss: 0.9934 - val_acc: 0.6484\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 1.0402 - acc: 0.6319 - val_loss: 1.1252 - val_acc: 0.6059\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 122s 97ms/step - loss: 0.9413 - acc: 0.6684 - val_loss: 0.9770 - val_acc: 0.6637\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.8763 - acc: 0.6915 - val_loss: 0.8933 - val_acc: 0.6970\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 118s 94ms/step - loss: 0.8228 - acc: 0.7124 - val_loss: 0.8771 - val_acc: 0.7022\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 119s 96ms/step - loss: 0.7768 - acc: 0.7288 - val_loss: 0.7971 - val_acc: 0.7302\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 116s 93ms/step - loss: 0.7501 - acc: 0.7385 - val_loss: 0.6393 - val_acc: 0.7755\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 116s 93ms/step - loss: 0.7190 - acc: 0.7495 - val_loss: 0.6844 - val_acc: 0.7639\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.6938 - acc: 0.7593 - val_loss: 0.8859 - val_acc: 0.7086\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.6709 - acc: 0.7703 - val_loss: 0.6993 - val_acc: 0.7537\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 122s 97ms/step - loss: 0.6519 - acc: 0.7743 - val_loss: 0.6231 - val_acc: 0.7875\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 122s 97ms/step - loss: 0.6370 - acc: 0.7787 - val_loss: 0.7004 - val_acc: 0.7599\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.6133 - acc: 0.7875 - val_loss: 0.6344 - val_acc: 0.7850\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.6026 - acc: 0.7938 - val_loss: 0.5520 - val_acc: 0.8139\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.5882 - acc: 0.7960 - val_loss: 0.6331 - val_acc: 0.7801\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.5811 - acc: 0.7979 - val_loss: 0.6915 - val_acc: 0.7704\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.5600 - acc: 0.8053 - val_loss: 0.7586 - val_acc: 0.7510\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.5601 - acc: 0.8072 - val_loss: 0.6929 - val_acc: 0.7678\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.5474 - acc: 0.8100 - val_loss: 0.5324 - val_acc: 0.8213\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.5372 - acc: 0.8146 - val_loss: 0.5796 - val_acc: 0.8013\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.5328 - acc: 0.8164 - val_loss: 0.4854 - val_acc: 0.8347\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.5215 - acc: 0.8182 - val_loss: 0.5091 - val_acc: 0.8293\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 128s 102ms/step - loss: 0.5155 - acc: 0.8205 - val_loss: 0.4466 - val_acc: 0.8477\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.5057 - acc: 0.8258 - val_loss: 0.4466 - val_acc: 0.8463\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.4941 - acc: 0.8293 - val_loss: 0.5127 - val_acc: 0.8281\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.4912 - acc: 0.8327 - val_loss: 0.5430 - val_acc: 0.8177\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 0.4832 - acc: 0.8339 - val_loss: 0.5942 - val_acc: 0.8040\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.4813 - acc: 0.8317 - val_loss: 0.4718 - val_acc: 0.8374\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.4727 - acc: 0.8362 - val_loss: 0.5064 - val_acc: 0.8327\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 114s 91ms/step - loss: 0.4683 - acc: 0.8388 - val_loss: 0.6165 - val_acc: 0.7955\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 116s 92ms/step - loss: 0.4650 - acc: 0.8385 - val_loss: 0.6279 - val_acc: 0.7915\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.4509 - acc: 0.8450 - val_loss: 0.4559 - val_acc: 0.8473\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.4446 - acc: 0.8466 - val_loss: 0.4987 - val_acc: 0.8362\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.4507 - acc: 0.8452 - val_loss: 0.5111 - val_acc: 0.8292\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.4473 - acc: 0.8457 - val_loss: 0.4724 - val_acc: 0.8430\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.4410 - acc: 0.8471 - val_loss: 0.4354 - val_acc: 0.8544\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.4387 - acc: 0.8489 - val_loss: 0.4302 - val_acc: 0.8515\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 122s 97ms/step - loss: 0.4330 - acc: 0.8509 - val_loss: 0.4959 - val_acc: 0.8331\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.4227 - acc: 0.8543 - val_loss: 0.4051 - val_acc: 0.8624\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 0.4214 - acc: 0.8539 - val_loss: 0.5601 - val_acc: 0.8172\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.4229 - acc: 0.8545 - val_loss: 0.4614 - val_acc: 0.8503\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 0.4184 - acc: 0.8563 - val_loss: 0.4743 - val_acc: 0.8366\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 127s 102ms/step - loss: 0.4139 - acc: 0.8581 - val_loss: 0.4137 - val_acc: 0.8616\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 126s 100ms/step - loss: 0.4146 - acc: 0.8582 - val_loss: 0.4902 - val_acc: 0.8385\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.4084 - acc: 0.8596 - val_loss: 0.4329 - val_acc: 0.8505\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.4062 - acc: 0.8593 - val_loss: 0.4167 - val_acc: 0.8625\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.4001 - acc: 0.8625 - val_loss: 0.4513 - val_acc: 0.8458\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.4022 - acc: 0.8628 - val_loss: 0.4095 - val_acc: 0.8620\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.3997 - acc: 0.8622 - val_loss: 0.4452 - val_acc: 0.8535\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.3954 - acc: 0.8631 - val_loss: 0.4398 - val_acc: 0.8548\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 118s 94ms/step - loss: 0.3850 - acc: 0.8663 - val_loss: 0.4428 - val_acc: 0.8515\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 116s 92ms/step - loss: 0.3844 - acc: 0.8688 - val_loss: 0.4761 - val_acc: 0.8425\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 118s 94ms/step - loss: 0.3844 - acc: 0.8672 - val_loss: 0.4740 - val_acc: 0.8464\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.3830 - acc: 0.8670 - val_loss: 0.4583 - val_acc: 0.8496\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3780 - acc: 0.8689 - val_loss: 0.4119 - val_acc: 0.8603\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 124s 100ms/step - loss: 0.3723 - acc: 0.8717 - val_loss: 0.4449 - val_acc: 0.8493\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.3821 - acc: 0.8675 - val_loss: 0.5256 - val_acc: 0.8278\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 0.3739 - acc: 0.8697 - val_loss: 0.4433 - val_acc: 0.8543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.3751 - acc: 0.8699 - val_loss: 0.3719 - val_acc: 0.8723\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.3663 - acc: 0.8737 - val_loss: 0.4782 - val_acc: 0.8443\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.3678 - acc: 0.8713 - val_loss: 0.4236 - val_acc: 0.8618\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.3627 - acc: 0.8748 - val_loss: 0.3981 - val_acc: 0.8640\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.3577 - acc: 0.8770 - val_loss: 0.4641 - val_acc: 0.8467\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 0.3607 - acc: 0.8741 - val_loss: 0.5626 - val_acc: 0.8253\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 0.3592 - acc: 0.8751 - val_loss: 0.4458 - val_acc: 0.8558\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3540 - acc: 0.8764 - val_loss: 0.3598 - val_acc: 0.8807\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3554 - acc: 0.8757 - val_loss: 0.4514 - val_acc: 0.8524\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3487 - acc: 0.8802 - val_loss: 0.4093 - val_acc: 0.8636\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3481 - acc: 0.8800 - val_loss: 0.4216 - val_acc: 0.8598\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3464 - acc: 0.8783 - val_loss: 0.4194 - val_acc: 0.8593\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3462 - acc: 0.8804 - val_loss: 0.4188 - val_acc: 0.8613\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3430 - acc: 0.8794 - val_loss: 0.3852 - val_acc: 0.8730\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3446 - acc: 0.8820 - val_loss: 0.4219 - val_acc: 0.8600\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3421 - acc: 0.8814 - val_loss: 0.4153 - val_acc: 0.8618\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3358 - acc: 0.8844 - val_loss: 0.3995 - val_acc: 0.8675\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3398 - acc: 0.8822 - val_loss: 0.4056 - val_acc: 0.8680\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3366 - acc: 0.8839 - val_loss: 0.3563 - val_acc: 0.8820\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3353 - acc: 0.8846 - val_loss: 0.4451 - val_acc: 0.8586\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3293 - acc: 0.8862 - val_loss: 0.4058 - val_acc: 0.8662\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3349 - acc: 0.8843 - val_loss: 0.4201 - val_acc: 0.8625\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3307 - acc: 0.8845 - val_loss: 0.4016 - val_acc: 0.8696\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3270 - acc: 0.8861 - val_loss: 0.3822 - val_acc: 0.8744\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3291 - acc: 0.8855 - val_loss: 0.4009 - val_acc: 0.8707\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3253 - acc: 0.8864 - val_loss: 0.4080 - val_acc: 0.8633\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3247 - acc: 0.8870 - val_loss: 0.4788 - val_acc: 0.8441\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3210 - acc: 0.8890 - val_loss: 0.4039 - val_acc: 0.8664\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3258 - acc: 0.8855 - val_loss: 0.4587 - val_acc: 0.8516\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3172 - acc: 0.8898 - val_loss: 0.4625 - val_acc: 0.8530\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3184 - acc: 0.8885 - val_loss: 0.3917 - val_acc: 0.8693\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3237 - acc: 0.8875 - val_loss: 0.4197 - val_acc: 0.8625\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3181 - acc: 0.8881 - val_loss: 0.4120 - val_acc: 0.8651\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3152 - acc: 0.8909 - val_loss: 0.3736 - val_acc: 0.8739\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3166 - acc: 0.8893 - val_loss: 0.3552 - val_acc: 0.8774\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3155 - acc: 0.8903 - val_loss: 0.3843 - val_acc: 0.8709\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3133 - acc: 0.8921 - val_loss: 0.3613 - val_acc: 0.8798\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3157 - acc: 0.8900 - val_loss: 0.4156 - val_acc: 0.8669\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3089 - acc: 0.8918 - val_loss: 0.3720 - val_acc: 0.8775\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3054 - acc: 0.8938 - val_loss: 0.3667 - val_acc: 0.8788\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3060 - acc: 0.8932 - val_loss: 0.4013 - val_acc: 0.8711\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_tr)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=32),\n",
    "                              steps_per_epoch=len(x_tr) / 32, epochs=200,\n",
    "                              validation_data=(x_val, y_val))\n",
    "\n",
    "# history = model.fit(x_tr, y_tr, batch_size=32, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_319 (Conv2D)          (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_144 (Activation)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_320 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_136 (MaxPoolin (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_321 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_322 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_137 (MaxPoolin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_323 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_324 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_138 (MaxPoolin (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 849,194\n",
      "Trainable params: 848,234\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final = Sequential()\n",
    "\n",
    "model_final.add(Conv2D(32, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model_final.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Conv2D(64, (3, 3),  padding='same'))\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model_final.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Dropout(rate=0.5))\n",
    "model_final.add(Conv2D(128, (3, 3),padding='same'))\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model_final.add(Flatten())\n",
    "\n",
    "model_final.add(Dropout(rate=0.5))\n",
    "model_final.add(Dense(256))\n",
    "model_final.add(BatchNormalization())\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Dense(128))\n",
    "model_final.add(Activation('relu'))\n",
    "model_final.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_final.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1562 [==============================] - 144s 92ms/step - loss: 1.8671 - acc: 0.3004\n",
      "Epoch 2/200\n",
      "1563/1562 [==============================] - 125s 80ms/step - loss: 1.5980 - acc: 0.4113\n",
      "Epoch 3/200\n",
      "1563/1562 [==============================] - 124s 79ms/step - loss: 1.4479 - acc: 0.4733\n",
      "Epoch 4/200\n",
      "1563/1562 [==============================] - 120s 77ms/step - loss: 1.3382 - acc: 0.5150\n",
      "Epoch 5/200\n",
      "1563/1562 [==============================] - 124s 79ms/step - loss: 1.2654 - acc: 0.5463\n",
      "Epoch 6/200\n",
      "1563/1562 [==============================] - 129s 83ms/step - loss: 1.2072 - acc: 0.5704\n",
      "Epoch 7/200\n",
      "1563/1562 [==============================] - 125s 80ms/step - loss: 1.1626 - acc: 0.5887\n",
      "Epoch 8/200\n",
      "1563/1562 [==============================] - 122s 78ms/step - loss: 1.1266 - acc: 0.6020\n",
      "Epoch 9/200\n",
      "1563/1562 [==============================] - 124s 79ms/step - loss: 1.0883 - acc: 0.6164\n",
      "Epoch 10/200\n",
      "1563/1562 [==============================] - 126s 80ms/step - loss: 1.0609 - acc: 0.6271\n",
      "Epoch 11/200\n",
      "1563/1562 [==============================] - 121s 78ms/step - loss: 1.0331 - acc: 0.6392\n",
      "Epoch 12/200\n",
      "1563/1562 [==============================] - 120s 76ms/step - loss: 1.0080 - acc: 0.6471\n",
      "Epoch 13/200\n",
      "1563/1562 [==============================] - 120s 77ms/step - loss: 0.9848 - acc: 0.6557\n",
      "Epoch 14/200\n",
      "1563/1562 [==============================] - 120s 77ms/step - loss: 0.9737 - acc: 0.6597\n",
      "Epoch 15/200\n",
      "1563/1562 [==============================] - 119s 76ms/step - loss: 0.9510 - acc: 0.6671\n",
      "Epoch 16/200\n",
      "1563/1562 [==============================] - 119s 76ms/step - loss: 0.9417 - acc: 0.6751\n",
      "Epoch 17/200\n",
      "1563/1562 [==============================] - 119s 76ms/step - loss: 0.9233 - acc: 0.6788\n",
      "Epoch 18/200\n",
      "1563/1562 [==============================] - 120s 77ms/step - loss: 0.9111 - acc: 0.6837\n",
      "Epoch 19/200\n",
      "1563/1562 [==============================] - 121s 78ms/step - loss: 0.8971 - acc: 0.6891\n",
      "Epoch 20/200\n",
      "1563/1562 [==============================] - 126s 81ms/step - loss: 0.8922 - acc: 0.6916\n",
      "Epoch 21/200\n",
      "1563/1562 [==============================] - 129s 83ms/step - loss: 0.8778 - acc: 0.6938\n",
      "Epoch 22/200\n",
      "1563/1562 [==============================] - 133s 85ms/step - loss: 0.8612 - acc: 0.7026\n",
      "Epoch 23/200\n",
      "1563/1562 [==============================] - 130s 83ms/step - loss: 0.8637 - acc: 0.7013\n",
      "Epoch 24/200\n",
      "1563/1562 [==============================] - 127s 82ms/step - loss: 0.8524 - acc: 0.7033\n",
      "Epoch 25/200\n",
      "1563/1562 [==============================] - 132s 84ms/step - loss: 0.8463 - acc: 0.7089\n",
      "Epoch 26/200\n",
      "1563/1562 [==============================] - 132s 84ms/step - loss: 0.8361 - acc: 0.7114\n",
      "Epoch 27/200\n",
      "1563/1562 [==============================] - 133s 85ms/step - loss: 0.8299 - acc: 0.7130\n",
      "Epoch 28/200\n",
      "1563/1562 [==============================] - 138s 89ms/step - loss: 0.8283 - acc: 0.7182\n",
      "Epoch 29/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.8169 - acc: 0.7186\n",
      "Epoch 30/200\n",
      "1563/1562 [==============================] - 140s 90ms/step - loss: 0.8135 - acc: 0.7193\n",
      "Epoch 31/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.8051 - acc: 0.7219\n",
      "Epoch 32/200\n",
      "1563/1562 [==============================] - 139s 89ms/step - loss: 0.8026 - acc: 0.7229\n",
      "Epoch 33/200\n",
      "1563/1562 [==============================] - 149s 96ms/step - loss: 0.7966 - acc: 0.7237\n",
      "Epoch 34/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.7900 - acc: 0.7304\n",
      "Epoch 35/200\n",
      "1563/1562 [==============================] - 155s 99ms/step - loss: 0.7889 - acc: 0.7292\n",
      "Epoch 36/200\n",
      "1563/1562 [==============================] - 160s 102ms/step - loss: 0.7771 - acc: 0.7314\n",
      "Epoch 37/200\n",
      "1563/1562 [==============================] - 156s 100ms/step - loss: 0.7770 - acc: 0.7314\n",
      "Epoch 38/200\n",
      "1563/1562 [==============================] - 154s 99ms/step - loss: 0.7758 - acc: 0.7326\n",
      "Epoch 39/200\n",
      "1563/1562 [==============================] - 157s 101ms/step - loss: 0.7652 - acc: 0.7356\n",
      "Epoch 40/200\n",
      "1563/1562 [==============================] - 158s 101ms/step - loss: 0.7665 - acc: 0.7367\n",
      "Epoch 41/200\n",
      "1563/1562 [==============================] - 154s 99ms/step - loss: 0.7581 - acc: 0.7380\n",
      "Epoch 42/200\n",
      "1563/1562 [==============================] - 154s 98ms/step - loss: 0.7587 - acc: 0.7373\n",
      "Epoch 43/200\n",
      "1563/1562 [==============================] - 155s 99ms/step - loss: 0.7534 - acc: 0.7392\n",
      "Epoch 44/200\n",
      "1563/1562 [==============================] - 154s 99ms/step - loss: 0.7499 - acc: 0.7413\n",
      "Epoch 45/200\n",
      "1563/1562 [==============================] - 153s 98ms/step - loss: 0.7503 - acc: 0.7428\n",
      "Epoch 46/200\n",
      "1563/1562 [==============================] - 153s 98ms/step - loss: 0.7446 - acc: 0.7432\n",
      "Epoch 47/200\n",
      "1563/1562 [==============================] - 161s 103ms/step - loss: 0.7422 - acc: 0.7443\n",
      "Epoch 48/200\n",
      "1563/1562 [==============================] - 164s 105ms/step - loss: 0.7353 - acc: 0.7473\n",
      "Epoch 49/200\n",
      "1563/1562 [==============================] - 169s 108ms/step - loss: 0.7382 - acc: 0.7447\n",
      "Epoch 50/200\n",
      "1563/1562 [==============================] - 164s 105ms/step - loss: 0.7356 - acc: 0.7461\n",
      "Epoch 51/200\n",
      "1563/1562 [==============================] - 160s 102ms/step - loss: 0.7286 - acc: 0.7500\n",
      "Epoch 52/200\n",
      "1563/1562 [==============================] - 158s 101ms/step - loss: 0.7241 - acc: 0.7525\n",
      "Epoch 53/200\n",
      "1563/1562 [==============================] - 155s 99ms/step - loss: 0.7279 - acc: 0.7484\n",
      "Epoch 54/200\n",
      "1563/1562 [==============================] - 150s 96ms/step - loss: 0.7219 - acc: 0.7512\n",
      "Epoch 55/200\n",
      "1563/1562 [==============================] - 146s 93ms/step - loss: 0.7210 - acc: 0.7509\n",
      "Epoch 56/200\n",
      "1563/1562 [==============================] - 150s 96ms/step - loss: 0.7108 - acc: 0.7576\n",
      "Epoch 57/200\n",
      "1563/1562 [==============================] - 151s 96ms/step - loss: 0.7135 - acc: 0.7561\n",
      "Epoch 58/200\n",
      "1563/1562 [==============================] - 156s 100ms/step - loss: 0.7061 - acc: 0.7565\n",
      "Epoch 59/200\n",
      "1563/1562 [==============================] - 159s 102ms/step - loss: 0.7078 - acc: 0.7558\n",
      "Epoch 60/200\n",
      "1563/1562 [==============================] - 161s 103ms/step - loss: 0.7097 - acc: 0.7570\n",
      "Epoch 61/200\n",
      "1563/1562 [==============================] - 157s 100ms/step - loss: 0.7098 - acc: 0.7548\n",
      "Epoch 62/200\n",
      "1563/1562 [==============================] - 160s 103ms/step - loss: 0.7019 - acc: 0.7583\n",
      "Epoch 63/200\n",
      "1563/1562 [==============================] - 161s 103ms/step - loss: 0.7020 - acc: 0.7583\n",
      "Epoch 64/200\n",
      "1563/1562 [==============================] - 157s 100ms/step - loss: 0.6994 - acc: 0.7596\n",
      "Epoch 65/200\n",
      "1563/1562 [==============================] - 154s 99ms/step - loss: 0.6952 - acc: 0.7608\n",
      "Epoch 66/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6937 - acc: 0.7620\n",
      "Epoch 67/200\n",
      "1563/1562 [==============================] - 154s 99ms/step - loss: 0.7003 - acc: 0.7611\n",
      "Epoch 68/200\n",
      "1563/1562 [==============================] - 147s 94ms/step - loss: 0.6904 - acc: 0.7622\n",
      "Epoch 69/200\n",
      "1563/1562 [==============================] - 145s 93ms/step - loss: 0.6896 - acc: 0.7617\n",
      "Epoch 70/200\n",
      "1563/1562 [==============================] - 151s 97ms/step - loss: 0.6915 - acc: 0.7624\n",
      "Epoch 71/200\n",
      "1563/1562 [==============================] - 152s 97ms/step - loss: 0.6856 - acc: 0.7653\n",
      "Epoch 72/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6827 - acc: 0.7653\n",
      "Epoch 73/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6760 - acc: 0.7677\n",
      "Epoch 74/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.6835 - acc: 0.7643\n",
      "Epoch 75/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.6746 - acc: 0.7663\n",
      "Epoch 76/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6769 - acc: 0.7668\n",
      "Epoch 77/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.6776 - acc: 0.7660\n",
      "Epoch 78/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6754 - acc: 0.7679\n",
      "Epoch 79/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.6710 - acc: 0.7704\n",
      "Epoch 80/200\n",
      "1563/1562 [==============================] - 138s 89ms/step - loss: 0.6654 - acc: 0.7712\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 139s 89ms/step - loss: 0.6628 - acc: 0.7718\n",
      "Epoch 82/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6690 - acc: 0.7714\n",
      "Epoch 83/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6617 - acc: 0.7712\n",
      "Epoch 84/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6610 - acc: 0.7725\n",
      "Epoch 85/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6630 - acc: 0.7727\n",
      "Epoch 86/200\n",
      "1563/1562 [==============================] - 145s 92ms/step - loss: 0.6629 - acc: 0.7709\n",
      "Epoch 87/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6618 - acc: 0.7722\n",
      "Epoch 88/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6622 - acc: 0.7724\n",
      "Epoch 89/200\n",
      "1563/1562 [==============================] - 147s 94ms/step - loss: 0.6609 - acc: 0.7740\n",
      "Epoch 90/200\n",
      "1563/1562 [==============================] - 144s 92ms/step - loss: 0.6523 - acc: 0.7743\n",
      "Epoch 91/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6527 - acc: 0.7761\n",
      "Epoch 92/200\n",
      "1563/1562 [==============================] - 146s 94ms/step - loss: 0.6546 - acc: 0.7755\n",
      "Epoch 93/200\n",
      "1563/1562 [==============================] - 143s 91ms/step - loss: 0.6519 - acc: 0.7778\n",
      "Epoch 94/200\n",
      "1563/1562 [==============================] - 145s 92ms/step - loss: 0.6475 - acc: 0.7761\n",
      "Epoch 95/200\n",
      "1563/1562 [==============================] - 146s 94ms/step - loss: 0.6443 - acc: 0.7778\n",
      "Epoch 96/200\n",
      "1563/1562 [==============================] - 144s 92ms/step - loss: 0.6488 - acc: 0.7779\n",
      "Epoch 97/200\n",
      "1563/1562 [==============================] - 147s 94ms/step - loss: 0.6497 - acc: 0.7759\n",
      "Epoch 98/200\n",
      "1563/1562 [==============================] - 147s 94ms/step - loss: 0.6472 - acc: 0.7779\n",
      "Epoch 99/200\n",
      "1563/1562 [==============================] - 144s 92ms/step - loss: 0.6404 - acc: 0.7778\n",
      "Epoch 100/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6495 - acc: 0.7771\n",
      "Epoch 101/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6455 - acc: 0.7770\n",
      "Epoch 102/200\n",
      "1563/1562 [==============================] - 152s 97ms/step - loss: 0.6398 - acc: 0.7781\n",
      "Epoch 103/200\n",
      "1563/1562 [==============================] - 151s 97ms/step - loss: 0.6456 - acc: 0.7780\n",
      "Epoch 104/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6338 - acc: 0.7811\n",
      "Epoch 105/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6341 - acc: 0.7825\n",
      "Epoch 106/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6324 - acc: 0.7815\n",
      "Epoch 107/200\n",
      "1563/1562 [==============================] - 149s 95ms/step - loss: 0.6405 - acc: 0.7798\n",
      "Epoch 108/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6359 - acc: 0.7823\n",
      "Epoch 109/200\n",
      "1563/1562 [==============================] - 147s 94ms/step - loss: 0.6371 - acc: 0.7802\n",
      "Epoch 110/200\n",
      "1563/1562 [==============================] - 150s 96ms/step - loss: 0.6348 - acc: 0.7821\n",
      "Epoch 111/200\n",
      "1563/1562 [==============================] - 148s 95ms/step - loss: 0.6346 - acc: 0.7821\n",
      "Epoch 112/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6318 - acc: 0.7837\n",
      "Epoch 113/200\n",
      "1563/1562 [==============================] - 145s 93ms/step - loss: 0.6323 - acc: 0.7842\n",
      "Epoch 114/200\n",
      "1563/1562 [==============================] - 145s 93ms/step - loss: 0.6310 - acc: 0.7831\n",
      "Epoch 115/200\n",
      "1563/1562 [==============================] - 140s 89ms/step - loss: 0.6277 - acc: 0.7830\n",
      "Epoch 116/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6255 - acc: 0.7832\n",
      "Epoch 117/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6274 - acc: 0.7846\n",
      "Epoch 118/200\n",
      "1563/1562 [==============================] - 143s 92ms/step - loss: 0.6249 - acc: 0.7845\n",
      "Epoch 119/200\n",
      "1563/1562 [==============================] - 141s 90ms/step - loss: 0.6234 - acc: 0.7868\n",
      "Epoch 120/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6277 - acc: 0.7815\n",
      "Epoch 121/200\n",
      "1563/1562 [==============================] - 141s 90ms/step - loss: 0.6225 - acc: 0.7859\n",
      "Epoch 122/200\n",
      "1563/1562 [==============================] - 141s 90ms/step - loss: 0.6251 - acc: 0.7844\n",
      "Epoch 123/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.6119 - acc: 0.7883\n",
      "Epoch 124/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6205 - acc: 0.7867\n",
      "Epoch 125/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6154 - acc: 0.7902\n",
      "Epoch 126/200\n",
      "1563/1562 [==============================] - 145s 93ms/step - loss: 0.6258 - acc: 0.7857\n",
      "Epoch 127/200\n",
      "1563/1562 [==============================] - 145s 93ms/step - loss: 0.6185 - acc: 0.7864\n",
      "Epoch 128/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6141 - acc: 0.7887\n",
      "Epoch 129/200\n",
      "1563/1562 [==============================] - 140s 90ms/step - loss: 0.6159 - acc: 0.7872\n",
      "Epoch 130/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6155 - acc: 0.7881\n",
      "Epoch 131/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6215 - acc: 0.7874\n",
      "Epoch 132/200\n",
      "1563/1562 [==============================] - 144s 92ms/step - loss: 0.6154 - acc: 0.7876\n",
      "Epoch 133/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.6142 - acc: 0.7894\n",
      "Epoch 134/200\n",
      "1563/1562 [==============================] - 139s 89ms/step - loss: 0.6134 - acc: 0.7898\n",
      "Epoch 135/200\n",
      "1563/1562 [==============================] - 142s 91ms/step - loss: 0.6101 - acc: 0.7922\n",
      "Epoch 136/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.6083 - acc: 0.7912\n",
      "Epoch 137/200\n",
      "1563/1562 [==============================] - 134s 86ms/step - loss: 0.6036 - acc: 0.7918\n",
      "Epoch 138/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6112 - acc: 0.7916\n",
      "Epoch 139/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.6144 - acc: 0.7880\n",
      "Epoch 140/200\n",
      "1563/1562 [==============================] - 141s 90ms/step - loss: 0.6074 - acc: 0.7913\n",
      "Epoch 141/200\n",
      "1563/1562 [==============================] - 134s 86ms/step - loss: 0.6072 - acc: 0.7912\n",
      "Epoch 142/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.6122 - acc: 0.7890\n",
      "Epoch 143/200\n",
      "1563/1562 [==============================] - 134s 86ms/step - loss: 0.6059 - acc: 0.7904\n",
      "Epoch 144/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6059 - acc: 0.7936\n",
      "Epoch 145/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6094 - acc: 0.7902\n",
      "Epoch 146/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.6015 - acc: 0.7917\n",
      "Epoch 147/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.6027 - acc: 0.7923\n",
      "Epoch 148/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6043 - acc: 0.7923\n",
      "Epoch 149/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6048 - acc: 0.7939\n",
      "Epoch 150/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.6070 - acc: 0.7907\n",
      "Epoch 151/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5940 - acc: 0.7955\n",
      "Epoch 152/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5914 - acc: 0.7967\n",
      "Epoch 153/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6008 - acc: 0.7932\n",
      "Epoch 154/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.5994 - acc: 0.7927\n",
      "Epoch 155/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6026 - acc: 0.7937\n",
      "Epoch 156/200\n",
      "1563/1562 [==============================] - 134s 86ms/step - loss: 0.5999 - acc: 0.7927\n",
      "Epoch 157/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6054 - acc: 0.7929\n",
      "Epoch 158/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5990 - acc: 0.7936\n",
      "Epoch 159/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.5988 - acc: 0.7936\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5922 - acc: 0.7951\n",
      "Epoch 161/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.5918 - acc: 0.7969\n",
      "Epoch 162/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.6008 - acc: 0.7953\n",
      "Epoch 163/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5928 - acc: 0.7960\n",
      "Epoch 164/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5955 - acc: 0.7950\n",
      "Epoch 165/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5918 - acc: 0.7968\n",
      "Epoch 166/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5916 - acc: 0.7959\n",
      "Epoch 167/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5935 - acc: 0.7939\n",
      "Epoch 168/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5892 - acc: 0.7984\n",
      "Epoch 169/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.5903 - acc: 0.7987\n",
      "Epoch 170/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5968 - acc: 0.7952\n",
      "Epoch 171/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.5933 - acc: 0.7948\n",
      "Epoch 172/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5884 - acc: 0.7986\n",
      "Epoch 173/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.5929 - acc: 0.7973\n",
      "Epoch 174/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5945 - acc: 0.7940\n",
      "Epoch 175/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.5921 - acc: 0.7983\n",
      "Epoch 176/200\n",
      "1563/1562 [==============================] - 138s 88ms/step - loss: 0.5894 - acc: 0.7985\n",
      "Epoch 177/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5828 - acc: 0.7992\n",
      "Epoch 178/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.5872 - acc: 0.7960\n",
      "Epoch 179/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5877 - acc: 0.7958\n",
      "Epoch 180/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.5831 - acc: 0.7985\n",
      "Epoch 181/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5802 - acc: 0.7996\n",
      "Epoch 182/200\n",
      "1563/1562 [==============================] - 135s 86ms/step - loss: 0.5803 - acc: 0.8021\n",
      "Epoch 183/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5755 - acc: 0.8011\n",
      "Epoch 184/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5817 - acc: 0.7976\n",
      "Epoch 185/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5795 - acc: 0.8006\n",
      "Epoch 186/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5780 - acc: 0.8016\n",
      "Epoch 187/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5831 - acc: 0.8000\n",
      "Epoch 188/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5807 - acc: 0.8019\n",
      "Epoch 189/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5779 - acc: 0.7995\n",
      "Epoch 190/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5762 - acc: 0.8011\n",
      "Epoch 191/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5779 - acc: 0.8024\n",
      "Epoch 192/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5827 - acc: 0.8006\n",
      "Epoch 193/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5861 - acc: 0.7997\n",
      "Epoch 194/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5743 - acc: 0.8029\n",
      "Epoch 195/200\n",
      "1563/1562 [==============================] - 135s 87ms/step - loss: 0.5774 - acc: 0.8033\n",
      "Epoch 196/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5783 - acc: 0.8005\n",
      "Epoch 197/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5763 - acc: 0.8005\n",
      "Epoch 198/200\n",
      "1563/1562 [==============================] - 137s 88ms/step - loss: 0.5700 - acc: 0.8025\n",
      "Epoch 199/200\n",
      "1563/1562 [==============================] - 137s 87ms/step - loss: 0.5816 - acc: 0.7985\n",
      "Epoch 200/200\n",
      "1563/1562 [==============================] - 136s 87ms/step - loss: 0.5810 - acc: 0.8012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1E-3\n",
    "\n",
    "model_final.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "history = model_final.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=32),\n",
    "                              steps_per_epoch=len(x_train) / 32, epochs=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 796us/step\n",
      "loss = 0.4341993057012558\n",
      "accuracy = 0.8609\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "loss = 0.39509083096981046\n",
      "accuracy = 0.8634\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = model_final.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
