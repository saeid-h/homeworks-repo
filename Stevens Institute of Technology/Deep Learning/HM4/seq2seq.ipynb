{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 4: Build a seq2seq model for machine translation.\n",
    "\n",
    "### Name: Saeid Hosseinipoor\n",
    "\n",
    "### Task: Translate English to Italian and Catalan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code.\n",
    "2. Complete the code in Section 1.1 and Section 4.2.\n",
    "\n",
    "    * Translation English to **German** is not acceptable!!! Try another language.\n",
    "    \n",
    "3. **Make improvements.** Directly modify the code in Section 3. Do at least one of the followings. By doing more, you will get up to 2 bonus scores to the total.\n",
    "\n",
    "    * Bi-LSTM instead of LSTM\n",
    "    \n",
    "    * Multi-task learning (e.g., both English to French and English to Spanish)\n",
    "    \n",
    "    * Attention\n",
    "    \n",
    "4. Evaluate the translation using the BLEU score. \n",
    "\n",
    "    * Optional. Up to 2 bonus scores to the total.\n",
    "    \n",
    "5. Convert the notebook to .HTML file. \n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "\n",
    "6. Put the .HTML file in your own Github repo. \n",
    "\n",
    "7. Submit the link to the HTML file to Canvas\n",
    "\n",
    "    * E.g., https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM4/seq2seq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: To implement Bi-LSTM, you will need the following code to build the encoder; the decoder won't be much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import LSTM, Bidirectional, Concatenate\n",
    "# \n",
    "# encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, \n",
    "#                                   dropout=0.5, name='encoder_lstm'))\n",
    "# _, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "# state_h = Concatenate()([forward_h, backward_h])\n",
    "# state_c = Concatenate()([forward_c, backward_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the following blanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "filename_it = 'Data/ita.txt'\n",
    "filename_ron = 'Data/ron.txt'\n",
    "filename_por = 'Data/por.txt'\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "# Total number is 321433\n",
    "n_train = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "doc_it = load_doc(filename_it)\n",
    "doc_por = load_doc(filename_por)\n",
    "doc_ron = load_doc(filename_ron)\n",
    "\n",
    "# split into Language1-Language2 pairs\n",
    "pairs_it = to_pairs(doc_it)\n",
    "# print (\"Total number of records: {:d}\".format(len(pairs)))\n",
    "pairs_ron = to_pairs(doc_ron)\n",
    "pairs_por = to_pairs(doc_por)\n",
    "\n",
    "# clean sentences\n",
    "clean_pairs_it = clean_data(pairs_it)[0:n_train, :]\n",
    "clean_pairs_por = clean_data(pairs_por)[0:n_train, :]\n",
    "clean_pairs_ron = clean_data(pairs_ron)[0:n_train, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English ====> Italian\n",
      "[im flabby] => [io sono fiacco]\n",
      "[im flabby] => [sono fiacca]\n",
      "[im flabby] => [io sono fiacca]\n",
      "[im for it] => [sono a favore]\n",
      "[im for it] => [io sono a favore]\n",
      "[im frugal] => [sono parsimonioso]\n",
      "[im greedy] => [sono avido]\n",
      "[im greedy] => [io sono avido]\n",
      "[im greedy] => [sono avida]\n",
      "[im greedy] => [io sono avida]\n",
      "\n",
      "English ====> Romanian\n",
      "[we cant all be like tom] => [nu putem fi toti ca tom]\n",
      "[we dont need your money] => [nu avem nevoie de banii tai]\n",
      "[we dont want to do that] => [nu vrem sa facem asta]\n",
      "[we have a house for rent] => [avem o casa de inchiriat]\n",
      "[we have only two dollars] => [noi avem doar doi dolari]\n",
      "[we have used up the coal] => [am folosit tot carbunele]\n",
      "[we heard the bomb go off] => [am auzit bomba explodand]\n",
      "[we must win at all costs] => [trebuie sa castigam cu orice pret]\n",
      "[we use shared facilities] => [folosim facilitati comune]\n",
      "[we want nothing from you] => [nu vrem nimic de la tine]\n",
      "\n",
      "English ====> Protuguese\n",
      "[you must go] => [voce deve ir]\n",
      "[you must go] => [tu deves partir]\n",
      "[you need me] => [voce precisa de mim]\n",
      "[you need me] => [voces precisam de mim]\n",
      "[you used me] => [voce me usou]\n",
      "[you woke me] => [voce me acordou]\n",
      "[youll lose] => [voce vai perder]\n",
      "[youre cute] => [voce e bonito]\n",
      "[youre cute] => [voce e bonita]\n",
      "[youre cute] => [voces sao bonitos]\n"
     ]
    }
   ],
   "source": [
    "print ('English ====> Italian')\n",
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs_it[i, 0] + '] => [' + clean_pairs_it[i, 1] + ']')\n",
    "\n",
    "print ('\\nEnglish ====> Romanian')\n",
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs_ron[i, 0] + '] => [' + clean_pairs_ron[i, 1] + ']')\n",
    "\n",
    "print ('\\nEnglish ====> Protuguese')\n",
    "for i in range(3000, 3010):\n",
    "    print('[' + clean_pairs_por[i, 0] + '] => [' + clean_pairs_por[i, 1] + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Italian input_texts:  (20000,)\n",
      "Length of Italian target_texts: (20000,)\n",
      "Length of Romanian input_texts:  (8215,)\n",
      "Length of Romanian target_texts: (8215,)\n",
      "Length of Portuguese input_texts:  (20000,)\n",
      "Length of Portuguese target_texts: (20000,)\n",
      "Length of all input_texts: (48215,)\n"
     ]
    }
   ],
   "source": [
    "input_texts_it = clean_pairs_it[:, 0]\n",
    "target_texts_it = ['\\t' + text + '\\n' for text in clean_pairs_it[:, 1]]\n",
    "\n",
    "input_texts_por = clean_pairs_por[:, 0]\n",
    "target_texts_por = ['\\t' + text + '\\n' for text in clean_pairs_por[:, 1]]\n",
    "\n",
    "input_texts_ron = clean_pairs_ron[:, 0]\n",
    "target_texts_ron = ['\\t' + text + '\\n' for text in clean_pairs_ron[:, 1]]\n",
    "\n",
    "input_texts_en = numpy.concatenate((input_texts_it, input_texts_por, input_texts_ron))\n",
    "\n",
    "print('Length of Italian input_texts:  ' + str(input_texts_it.shape))\n",
    "print('Length of Italian target_texts: ' + str(input_texts_it.shape))\n",
    "print('Length of Romanian input_texts:  ' + str(input_texts_ron.shape))\n",
    "print('Length of Romanian target_texts: ' + str(input_texts_ron.shape))\n",
    "print('Length of Portuguese input_texts:  ' + str(input_texts_por.shape))\n",
    "print('Length of Portuguese target_texts: ' + str(input_texts_por.shape))\n",
    "print ('Length of all input_texts: ' + str(input_texts_en.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of English input  sentences: 203\n",
      "max length of Italian target sentences: 41\n",
      "max length of Romanian target sentences: 232\n",
      "max length of Portuguese target sentences: 43\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max(len(line) for line in input_texts_en)\n",
    "max_decoder_seq_length_it = max(len(line) for line in target_texts_it)\n",
    "max_decoder_seq_length_por = max(len(line) for line in target_texts_por)\n",
    "max_decoder_seq_length_ron = max(len(line) for line in target_texts_ron)\n",
    "\n",
    "print('max length of English input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of Italian target sentences: %d' % (max_decoder_seq_length_it))\n",
    "print('max length of Romanian target sentences: %d' % (max_decoder_seq_length_ron))\n",
    "print('max length of Portuguese target sentences: %d' % (max_decoder_seq_length_por))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (20000, 203)\n",
      "shape of input_token_index: 27\n",
      "shape of input_token_index: 27\n",
      "shape of input_token_index: 27\n",
      "shape of Italian decoder_input_seq: (20000, 41)\n",
      "shape of Italian target_token_index: 29\n",
      "shape of Romanian decoder_input_seq: (8215, 232)\n",
      "shape of Romanina target_token_index: 28\n",
      "shape of Portugese decoder_input_seq: (20000, 43)\n",
      "shape of Portugese target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "\n",
    "encoder_input_seq_it, input_token_index_it = text2sequences(max_encoder_seq_length, input_texts_it)\n",
    "encoder_input_seq_ron, input_token_index_ron = text2sequences(max_encoder_seq_length, input_texts_ron)\n",
    "encoder_input_seq_por, input_token_index_por = text2sequences(max_encoder_seq_length, input_texts_por)\n",
    "\n",
    "decoder_input_seq_it, target_token_index_it = text2sequences(max_decoder_seq_length_it, target_texts_it)\n",
    "decoder_input_seq_ron, target_token_index_ron = text2sequences(max_decoder_seq_length_ron, target_texts_ron)\n",
    "decoder_input_seq_por, target_token_index_por = text2sequences(max_decoder_seq_length_por, target_texts_por)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq_it.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index_it)))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index_por)))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index_ron)))\n",
    "print('shape of Italian decoder_input_seq: ' + str(decoder_input_seq_it.shape))\n",
    "print('shape of Italian target_token_index: ' + str(len(target_token_index_it)))\n",
    "print('shape of Romanian decoder_input_seq: ' + str(decoder_input_seq_ron.shape))\n",
    "print('shape of Romanina target_token_index: ' + str(len(target_token_index_ron)))\n",
    "print('shape of Portugese decoder_input_seq: ' + str(decoder_input_seq_por.shape))\n",
    "print('shape of Portugese target_token_index: ' + str(len(target_token_index_por)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens_it: 30\n",
      "num_decoder_tokens_ron: 29\n",
      "num_decoder_tokens_por: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index_it) + 1\n",
    "num_decoder_tokens_it = len(target_token_index_it) + 1\n",
    "num_decoder_tokens_ron = len(target_token_index_ron) + 1\n",
    "num_decoder_tokens_por = len(target_token_index_por) + 1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens_it: ' + str(num_decoder_tokens_it))\n",
    "print('num_decoder_tokens_ron: ' + str(num_decoder_tokens_ron))\n",
    "print('num_decoder_tokens_por: ' + str(num_decoder_tokens_por))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tlavoro ai ferri\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts_it[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 12,  3, 18,  1, 11,  1,  2,  3,  4,  2, 22,  5, 11, 11,  4,  7,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_seq_it[100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 203, 28)\n",
      "(8215, 203, 28)\n",
      "(20000, 203, 28)\n",
      "(20000, 41, 30)\n",
      "(8215, 232, 29)\n",
      "(20000, 43, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "encoder_input_data_it = onehot_encode(encoder_input_seq_it, max_encoder_seq_length, num_encoder_tokens)\n",
    "encoder_input_data_ron = onehot_encode(encoder_input_seq_ron, max_encoder_seq_length, num_encoder_tokens)\n",
    "encoder_input_data_por = onehot_encode(encoder_input_seq_por, max_encoder_seq_length, num_encoder_tokens)\n",
    "\n",
    "decoder_input_data_it = onehot_encode(decoder_input_seq_it, max_decoder_seq_length_it, num_decoder_tokens_it)\n",
    "decoder_input_data_ron = onehot_encode(decoder_input_seq_ron, max_decoder_seq_length_ron, num_decoder_tokens_ron)\n",
    "decoder_input_data_por = onehot_encode(decoder_input_seq_por, max_decoder_seq_length_por, num_decoder_tokens_por)\n",
    "\n",
    "decoder_target_seq_it = numpy.zeros(decoder_input_seq_it.shape)\n",
    "decoder_target_seq_it[:, 0:-1] = decoder_input_seq_it[:, 1:]\n",
    "decoder_target_data_it = onehot_encode(decoder_target_seq_it, \n",
    "                                    max_decoder_seq_length_it, \n",
    "                                    num_decoder_tokens_it)\n",
    "\n",
    "decoder_target_seq_ron = numpy.zeros(decoder_input_seq_ron.shape)\n",
    "decoder_target_seq_ron[:, 0:-1] = decoder_input_seq_ron[:, 1:]\n",
    "decoder_target_data_ron = onehot_encode(decoder_target_seq_ron, \n",
    "                                    max_decoder_seq_length_ron, \n",
    "                                    num_decoder_tokens_ron)\n",
    "\n",
    "decoder_target_seq_por = numpy.zeros(decoder_input_seq_por.shape)\n",
    "decoder_target_seq_por[:, 0:-1] = decoder_input_seq_por[:, 1:]\n",
    "decoder_target_data_por = onehot_encode(decoder_target_seq_por, \n",
    "                                    max_decoder_seq_length_por, \n",
    "                                    num_decoder_tokens_por)\n",
    "\n",
    "print(encoder_input_data_it.shape)\n",
    "print(encoder_input_data_ron.shape)\n",
    "print(encoder_input_data_por.shape)\n",
    "print(decoder_input_data_it.shape)\n",
    "print(decoder_input_data_ron.shape)\n",
    "print(decoder_input_data_por.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_{t-1}$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saeid/.virtualenvs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/saeid/.virtualenvs/py3env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Bidirectional, Concatenate\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "# encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "#                     dropout=0.5, name='encoder_lstm')\n",
    "# _, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.5, name='encoder_lstm'))\n",
    "_, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 512), (None, 583680      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h_it = Input(shape=(2*latent_dim,), name='decoder_input_h_it')\n",
    "decoder_input_c_it = Input(shape=(2*latent_dim,), name='decoder_input_c_it')\n",
    "decoder_input_x_it = Input(shape=(None, num_decoder_tokens_it), name='decoder_input_x_it')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm_it = LSTM(2*latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm_it')\n",
    "decoder_lstm_outputs_it, state_h_it, state_c_it = decoder_lstm_it(decoder_input_x_it, \n",
    "                                                      initial_state=[decoder_input_h_it, decoder_input_c_it])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense_it = Dense(num_decoder_tokens_it, activation='softmax', name='decoder_dense_it')\n",
    "decoder_outputs_it = decoder_dense_it(decoder_lstm_outputs_it)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model_it = Model(inputs=[decoder_input_x_it, decoder_input_h_it, decoder_input_c_it],\n",
    "                      outputs=[decoder_outputs_it, state_h_it, state_c_it],\n",
    "                      name='decoder_it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x_it (InputLayer) (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h_it (InputLayer) (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c_it (InputLayer) (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_it (LSTM)          [(None, None, 512),  1112064     decoder_input_x_it[0][0]         \n",
      "                                                                 decoder_input_h_it[0][0]         \n",
      "                                                                 decoder_input_c_it[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_it (Dense)        (None, None, 30)     15390       decoder_lstm_it[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,127,454\n",
      "Trainable params: 1,127,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(decoder_model_it, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model_it, show_shapes=False,\n",
    "    to_file='decoder_it.pdf'\n",
    ")\n",
    "\n",
    "decoder_model_it.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs of the decoder network\n",
    "decoder_input_h_ron = Input(shape=(2*latent_dim,), name='decoder_input_h_ron')\n",
    "decoder_input_c_ron = Input(shape=(2*latent_dim,), name='decoder_input_c_ron')\n",
    "decoder_input_x_ron = Input(shape=(None, num_decoder_tokens_ron), name='decoder_input_x_ron')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm_ron = LSTM(2*latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm_ron')\n",
    "decoder_lstm_outputs_ron, state_h_ron, state_c_ron = decoder_lstm_ron(decoder_input_x_ron, \n",
    "                                                      initial_state=[decoder_input_h_ron, decoder_input_c_ron])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense_ron = Dense(num_decoder_tokens_ron, activation='softmax', name='decoder_dense_ron')\n",
    "decoder_outputs_ron = decoder_dense_ron(decoder_lstm_outputs_ron)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model_ron = Model(inputs=[decoder_input_x_ron, decoder_input_h_ron, decoder_input_c_ron],\n",
    "                      outputs=[decoder_outputs_ron, state_h_ron, state_c_ron],\n",
    "                      name='decoder_ron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs of the decoder network\n",
    "decoder_input_h_por = Input(shape=(2*latent_dim,), name='decoder_input_h_por')\n",
    "decoder_input_c_por = Input(shape=(2*latent_dim,), name='decoder_input_c_por')\n",
    "decoder_input_x_por = Input(shape=(None, num_decoder_tokens_por), name='decoder_input_x_por')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm_por = LSTM(2*latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm_por')\n",
    "decoder_lstm_outputs_por, state_h_por, state_c_por = decoder_lstm_por(decoder_input_x_por, \n",
    "                                                      initial_state=[decoder_input_h_por, decoder_input_c_por])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense_por = Dense(num_decoder_tokens_por, activation='softmax', name='decoder_dense_por')\n",
    "decoder_outputs_por = decoder_dense_por(decoder_lstm_outputs_por)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model_por = Model(inputs=[decoder_input_x_por, decoder_input_h_por, decoder_input_c_por],\n",
    "                      outputs=[decoder_outputs_por, state_h_por, state_c_por],\n",
    "                      name='decoder_por')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "\n",
    "decoder_input_x_it = Input(shape=(None, num_decoder_tokens_it), name='decoder_input_x_it')\n",
    "decoder_input_x_ron = Input(shape=(None,num_decoder_tokens_ron), name='decoder_input_x_ron')\n",
    "decoder_input_x_por = Input(shape=(None, num_decoder_tokens_por), name='decoder_input_x_por')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "\n",
    "decoder_lstm_output_it, _, _ = decoder_lstm_it(decoder_input_x_it, initial_state=encoder_final_states)\n",
    "decoder_pred_it = decoder_dense_it(decoder_lstm_output_it)\n",
    "\n",
    "decoder_lstm_output_ron, _, _ = decoder_lstm_ron(decoder_input_x_ron, initial_state=encoder_final_states)\n",
    "decoder_pred_ron = decoder_dense_ron(decoder_lstm_output_ron)\n",
    "\n",
    "decoder_lstm_output_por, _, _ = decoder_lstm_por(decoder_input_x_por, initial_state=encoder_final_states)\n",
    "decoder_pred_por = decoder_dense_por(decoder_lstm_output_por)\n",
    "\n",
    "model_it = Model(inputs=[encoder_input_x, decoder_input_x_it], outputs=decoder_pred_it, name='model_training_it')\n",
    "model_ron = Model(inputs=[encoder_input_x, decoder_input_x_ron], outputs=decoder_pred_ron, name='model_training_ron')\n",
    "model_por = Model(inputs=[encoder_input_x, decoder_input_x_por], outputs=decoder_pred_por, name='model_training_por')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"decoder_lstm_it/while/Exit_2:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"decoder_input_h_it:0\", shape=(?, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(state_h_it)\n",
    "print(decoder_input_h_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x_it (InputLayer) (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 512), (None, 583680      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_it (LSTM)          [(None, None, 512),  1112064     decoder_input_x_it[0][0]         \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense_it (Dense)        (None, None, 30)     15390       decoder_lstm_it[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,711,134\n",
      "Trainable params: 1,711,134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model_it, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=model_it, show_shapes=False,\n",
    "    to_file='model_training_it.pdf'\n",
    ")\n",
    "\n",
    "model_it.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_data(20000, 203, 28)\n",
      "shape of decoder_input_data(20000, 41, 30)\n",
      "shape of decoder_target_data(20000, 41, 30)\n"
     ]
    }
   ],
   "source": [
    "print('shape of encoder_input_data' + str(encoder_input_data_it.shape))\n",
    "print('shape of decoder_input_data' + str(decoder_input_data_it.shape))\n",
    "print('shape of decoder_target_data' + str(decoder_target_data_it.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saeid/.virtualenvs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/saeid/.virtualenvs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 1.2067 - val_loss: 1.0512\n",
      "Epoch 2/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.8940 - val_loss: 0.9112\n",
      "Epoch 3/50\n",
      "16000/16000 [==============================] - 138s 9ms/step - loss: 0.7903 - val_loss: 0.8252\n",
      "Epoch 4/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.7162 - val_loss: 0.7700\n",
      "Epoch 5/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.6607 - val_loss: 0.7212\n",
      "Epoch 6/50\n",
      "16000/16000 [==============================] - 150s 9ms/step - loss: 0.6178 - val_loss: 0.6807\n",
      "Epoch 7/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.5797 - val_loss: 0.6552\n",
      "Epoch 8/50\n",
      "16000/16000 [==============================] - 136s 8ms/step - loss: 0.5481 - val_loss: 0.6271\n",
      "Epoch 9/50\n",
      "16000/16000 [==============================] - 134s 8ms/step - loss: 0.5214 - val_loss: 0.6054\n",
      "Epoch 10/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.4965 - val_loss: 0.5820\n",
      "Epoch 11/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.4767 - val_loss: 0.5715\n",
      "Epoch 12/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.4603 - val_loss: 0.5534\n",
      "Epoch 13/50\n",
      "16000/16000 [==============================] - 136s 8ms/step - loss: 0.4410 - val_loss: 0.5335\n",
      "Epoch 14/50\n",
      "16000/16000 [==============================] - 135s 8ms/step - loss: 0.4264 - val_loss: 0.5334\n",
      "Epoch 15/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.4545 - val_loss: 0.5282\n",
      "Epoch 16/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.4725 - val_loss: 0.6509\n",
      "Epoch 17/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.4506 - val_loss: 0.5861\n",
      "Epoch 18/50\n",
      "16000/16000 [==============================] - 135s 8ms/step - loss: 0.4343 - val_loss: 0.5652\n",
      "Epoch 19/50\n",
      "16000/16000 [==============================] - 136s 9ms/step - loss: 0.4137 - val_loss: 0.5010\n",
      "Epoch 20/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.3642 - val_loss: 0.4878\n",
      "Epoch 21/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.3521 - val_loss: 0.4856\n",
      "Epoch 22/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.3412 - val_loss: 0.4788\n",
      "Epoch 23/50\n",
      "16000/16000 [==============================] - 134s 8ms/step - loss: 0.3314 - val_loss: 0.4800\n",
      "Epoch 24/50\n",
      "16000/16000 [==============================] - 138s 9ms/step - loss: 0.3224 - val_loss: 0.4758\n",
      "Epoch 25/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.3149 - val_loss: 0.4708\n",
      "Epoch 26/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.3202 - val_loss: 0.4739\n",
      "Epoch 27/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.3032 - val_loss: 0.4726\n",
      "Epoch 28/50\n",
      "16000/16000 [==============================] - 132s 8ms/step - loss: 0.2983 - val_loss: 0.4717\n",
      "Epoch 29/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.2852 - val_loss: 0.4689\n",
      "Epoch 30/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.2791 - val_loss: 0.4719\n",
      "Epoch 31/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2731 - val_loss: 0.4668\n",
      "Epoch 32/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.2671 - val_loss: 0.4708\n",
      "Epoch 33/50\n",
      "16000/16000 [==============================] - 132s 8ms/step - loss: 0.2594 - val_loss: 0.4687\n",
      "Epoch 34/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2536 - val_loss: 0.4701\n",
      "Epoch 35/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2510 - val_loss: 0.4693\n",
      "Epoch 36/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.2432 - val_loss: 0.4734\n",
      "Epoch 37/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.2395 - val_loss: 0.4724\n",
      "Epoch 38/50\n",
      "16000/16000 [==============================] - 134s 8ms/step - loss: 0.2357 - val_loss: 0.4782\n",
      "Epoch 39/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2317 - val_loss: 0.4775\n",
      "Epoch 40/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2256 - val_loss: 0.4795\n",
      "Epoch 41/50\n",
      "16000/16000 [==============================] - 141s 9ms/step - loss: 0.2204 - val_loss: 0.4807\n",
      "Epoch 42/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.2163 - val_loss: 0.4904\n",
      "Epoch 43/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.2329 - val_loss: 0.4816\n",
      "Epoch 44/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.2100 - val_loss: 0.4824\n",
      "Epoch 45/50\n",
      "16000/16000 [==============================] - 151s 9ms/step - loss: 0.2059 - val_loss: 0.4879\n",
      "Epoch 46/50\n",
      "16000/16000 [==============================] - 153s 10ms/step - loss: 0.2034 - val_loss: 0.4874\n",
      "Epoch 47/50\n",
      "16000/16000 [==============================] - 151s 9ms/step - loss: 0.1985 - val_loss: 0.4919\n",
      "Epoch 48/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.1948 - val_loss: 0.4896\n",
      "Epoch 49/50\n",
      "16000/16000 [==============================] - 155s 10ms/step - loss: 0.1961 - val_loss: 0.4943\n",
      "Epoch 50/50\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.1874 - val_loss: 0.4978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdedffa77f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_por.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model_por.fit([encoder_input_data_por, decoder_input_data_por],  # training data\n",
    "          decoder_target_data_por,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "# model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6572 samples, validate on 1643 samples\n",
      "Epoch 1/50\n",
      "6572/6572 [==============================] - 138s 21ms/step - loss: 0.4939 - val_loss: 0.8526\n",
      "Epoch 2/50\n",
      "6572/6572 [==============================] - 131s 20ms/step - loss: 0.3593 - val_loss: 0.9033\n",
      "Epoch 3/50\n",
      "6572/6572 [==============================] - 125s 19ms/step - loss: 0.3137 - val_loss: 0.6125\n",
      "Epoch 4/50\n",
      "6572/6572 [==============================] - 129s 20ms/step - loss: 0.3031 - val_loss: 0.5641\n",
      "Epoch 5/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.2858 - val_loss: 0.5384\n",
      "Epoch 6/50\n",
      "6572/6572 [==============================] - 138s 21ms/step - loss: 0.2741 - val_loss: 0.5152\n",
      "Epoch 7/50\n",
      "6572/6572 [==============================] - 138s 21ms/step - loss: 0.2628 - val_loss: 0.5018\n",
      "Epoch 8/50\n",
      "6572/6572 [==============================] - 131s 20ms/step - loss: 0.2556 - val_loss: 0.4899\n",
      "Epoch 9/50\n",
      "6572/6572 [==============================] - 129s 20ms/step - loss: 0.2464 - val_loss: 0.4826\n",
      "Epoch 10/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.2429 - val_loss: 0.4738\n",
      "Epoch 11/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.2344 - val_loss: 0.4672\n",
      "Epoch 12/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.2325 - val_loss: 0.4611\n",
      "Epoch 13/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.2273 - val_loss: 0.4587\n",
      "Epoch 14/50\n",
      "6572/6572 [==============================] - 127s 19ms/step - loss: 0.2206 - val_loss: 0.4513\n",
      "Epoch 15/50\n",
      "6572/6572 [==============================] - 130s 20ms/step - loss: 0.2164 - val_loss: 0.4442\n",
      "Epoch 16/50\n",
      "6572/6572 [==============================] - 131s 20ms/step - loss: 0.2125 - val_loss: 0.4370\n",
      "Epoch 17/50\n",
      "6572/6572 [==============================] - 131s 20ms/step - loss: 0.2077 - val_loss: 0.4315\n",
      "Epoch 18/50\n",
      "6572/6572 [==============================] - 130s 20ms/step - loss: 0.2042 - val_loss: 0.4280\n",
      "Epoch 19/50\n",
      "6572/6572 [==============================] - 123s 19ms/step - loss: 0.2031 - val_loss: 0.4247\n",
      "Epoch 20/50\n",
      "6572/6572 [==============================] - 127s 19ms/step - loss: 0.1975 - val_loss: 0.4224\n",
      "Epoch 21/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.1947 - val_loss: 0.4160\n",
      "Epoch 22/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.1910 - val_loss: 0.4133\n",
      "Epoch 23/50\n",
      "6572/6572 [==============================] - 133s 20ms/step - loss: 0.1880 - val_loss: 0.4156\n",
      "Epoch 24/50\n",
      "6572/6572 [==============================] - 129s 20ms/step - loss: 0.1842 - val_loss: 0.4055\n",
      "Epoch 25/50\n",
      "6572/6572 [==============================] - 125s 19ms/step - loss: 0.1823 - val_loss: 0.4110\n",
      "Epoch 26/50\n",
      "6572/6572 [==============================] - 137s 21ms/step - loss: 0.1784 - val_loss: 0.4029\n",
      "Epoch 27/50\n",
      "6572/6572 [==============================] - 131s 20ms/step - loss: 0.1754 - val_loss: 0.3983\n",
      "Epoch 28/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.1730 - val_loss: 0.3975\n",
      "Epoch 29/50\n",
      "6572/6572 [==============================] - 133s 20ms/step - loss: 0.1702 - val_loss: 0.3983\n",
      "Epoch 30/50\n",
      "6572/6572 [==============================] - 126s 19ms/step - loss: 0.1674 - val_loss: 0.3924\n",
      "Epoch 31/50\n",
      "6572/6572 [==============================] - 133s 20ms/step - loss: 0.1648 - val_loss: 0.3955\n",
      "Epoch 32/50\n",
      "6572/6572 [==============================] - 135s 21ms/step - loss: 0.1616 - val_loss: 0.3936\n",
      "Epoch 33/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.1592 - val_loss: 0.3936\n",
      "Epoch 34/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.1571 - val_loss: 0.3890\n",
      "Epoch 35/50\n",
      "6572/6572 [==============================] - 130s 20ms/step - loss: 0.1545 - val_loss: 0.3886\n",
      "Epoch 36/50\n",
      "6572/6572 [==============================] - 133s 20ms/step - loss: 0.1522 - val_loss: 0.3923\n",
      "Epoch 37/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.1508 - val_loss: 0.3894\n",
      "Epoch 38/50\n",
      "6572/6572 [==============================] - 137s 21ms/step - loss: 0.1482 - val_loss: 0.3889\n",
      "Epoch 39/50\n",
      "6572/6572 [==============================] - 136s 21ms/step - loss: 0.1459 - val_loss: 0.3889\n",
      "Epoch 40/50\n",
      "6572/6572 [==============================] - 134s 20ms/step - loss: 0.1438 - val_loss: 0.3871\n",
      "Epoch 41/50\n",
      "6572/6572 [==============================] - 122s 19ms/step - loss: 0.1415 - val_loss: 0.3881\n",
      "Epoch 42/50\n",
      "6572/6572 [==============================] - 128s 20ms/step - loss: 0.1394 - val_loss: 0.3920\n",
      "Epoch 43/50\n",
      "6572/6572 [==============================] - 142s 22ms/step - loss: 0.1371 - val_loss: 0.3886\n",
      "Epoch 44/50\n",
      "6572/6572 [==============================] - 138s 21ms/step - loss: 0.1354 - val_loss: 0.3905\n",
      "Epoch 45/50\n",
      "6572/6572 [==============================] - 146s 22ms/step - loss: 0.1334 - val_loss: 0.3911\n",
      "Epoch 46/50\n",
      "6572/6572 [==============================] - 126s 19ms/step - loss: 0.1315 - val_loss: 0.3988\n",
      "Epoch 47/50\n",
      "6572/6572 [==============================] - 139s 21ms/step - loss: 0.1298 - val_loss: 0.3962\n",
      "Epoch 48/50\n",
      "6572/6572 [==============================] - 146s 22ms/step - loss: 0.1277 - val_loss: 0.3927\n",
      "Epoch 49/50\n",
      "6572/6572 [==============================] - 138s 21ms/step - loss: 0.1262 - val_loss: 0.4015\n",
      "Epoch 50/50\n",
      "6572/6572 [==============================] - 137s 21ms/step - loss: 0.1243 - val_loss: 0.3977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fded90f0978>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ron.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model_ron.fit([encoder_input_data_ron, decoder_input_data_ron],  # training data\n",
    "          decoder_target_data_ron,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "# model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/50\n",
      "16000/16000 [==============================] - 132s 8ms/step - loss: 1.0503 - val_loss: 0.8983\n",
      "Epoch 2/50\n",
      "16000/16000 [==============================] - 129s 8ms/step - loss: 0.7731 - val_loss: 0.7856\n",
      "Epoch 3/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.6750 - val_loss: 0.7240\n",
      "Epoch 4/50\n",
      "16000/16000 [==============================] - 138s 9ms/step - loss: 0.6155 - val_loss: 0.6776\n",
      "Epoch 5/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.5658 - val_loss: 0.6348\n",
      "Epoch 6/50\n",
      "16000/16000 [==============================] - 144s 9ms/step - loss: 0.5240 - val_loss: 0.6108\n",
      "Epoch 7/50\n",
      "16000/16000 [==============================] - 142s 9ms/step - loss: 0.4864 - val_loss: 0.5857\n",
      "Epoch 8/50\n",
      "16000/16000 [==============================] - 142s 9ms/step - loss: 0.4578 - val_loss: 0.5722\n",
      "Epoch 9/50\n",
      "16000/16000 [==============================] - 136s 8ms/step - loss: 0.4278 - val_loss: 0.5575\n",
      "Epoch 10/50\n",
      "16000/16000 [==============================] - 134s 8ms/step - loss: 0.4035 - val_loss: 0.5564\n",
      "Epoch 11/50\n",
      "16000/16000 [==============================] - 134s 8ms/step - loss: 0.3822 - val_loss: 0.5469\n",
      "Epoch 12/50\n",
      "16000/16000 [==============================] - 127s 8ms/step - loss: 0.3608 - val_loss: 0.5375\n",
      "Epoch 13/50\n",
      "16000/16000 [==============================] - 129s 8ms/step - loss: 0.3430 - val_loss: 0.5422\n",
      "Epoch 14/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.3252 - val_loss: 0.5420\n",
      "Epoch 15/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.3094 - val_loss: 0.5405\n",
      "Epoch 16/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.2944 - val_loss: 0.5492\n",
      "Epoch 17/50\n",
      "16000/16000 [==============================] - 150s 9ms/step - loss: 0.2823 - val_loss: 0.5502\n",
      "Epoch 18/50\n",
      "16000/16000 [==============================] - 138s 9ms/step - loss: 0.2711 - val_loss: 0.5521\n",
      "Epoch 19/50\n",
      "16000/16000 [==============================] - 144s 9ms/step - loss: 0.2586 - val_loss: 0.5595\n",
      "Epoch 20/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.2507 - val_loss: 0.5522\n",
      "Epoch 21/50\n",
      "16000/16000 [==============================] - 144s 9ms/step - loss: 0.2385 - val_loss: 0.5614\n",
      "Epoch 22/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.2327 - val_loss: 0.5576\n",
      "Epoch 23/50\n",
      "16000/16000 [==============================] - 142s 9ms/step - loss: 0.2215 - val_loss: 0.5693\n",
      "Epoch 24/50\n",
      "16000/16000 [==============================] - 137s 9ms/step - loss: 0.2150 - val_loss: 0.5728\n",
      "Epoch 25/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.2082 - val_loss: 0.5697\n",
      "Epoch 26/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.2018 - val_loss: 0.5658\n",
      "Epoch 27/50\n",
      "16000/16000 [==============================] - 142s 9ms/step - loss: 0.1946 - val_loss: 0.5781\n",
      "Epoch 28/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.2189 - val_loss: 0.5714\n",
      "Epoch 29/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.1870 - val_loss: 0.5745\n",
      "Epoch 30/50\n",
      "16000/16000 [==============================] - 139s 9ms/step - loss: 0.1809 - val_loss: 0.5692\n",
      "Epoch 31/50\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.1752 - val_loss: 0.5844\n",
      "Epoch 32/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.1704 - val_loss: 0.5934\n",
      "Epoch 33/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.1681 - val_loss: 0.5902\n",
      "Epoch 34/50\n",
      "16000/16000 [==============================] - 145s 9ms/step - loss: 0.1628 - val_loss: 0.5977\n",
      "Epoch 35/50\n",
      "16000/16000 [==============================] - 137s 9ms/step - loss: 0.1627 - val_loss: 0.6040\n",
      "Epoch 36/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.1554 - val_loss: 0.6033\n",
      "Epoch 37/50\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.1517 - val_loss: 0.6108\n",
      "Epoch 38/50\n",
      "16000/16000 [==============================] - 149s 9ms/step - loss: 0.1488 - val_loss: 0.6083\n",
      "Epoch 39/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.1464 - val_loss: 0.6073\n",
      "Epoch 40/50\n",
      "16000/16000 [==============================] - 140s 9ms/step - loss: 0.1451 - val_loss: 0.6151\n",
      "Epoch 41/50\n",
      "16000/16000 [==============================] - 137s 9ms/step - loss: 0.1393 - val_loss: 0.6239\n",
      "Epoch 42/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.1440 - val_loss: 0.6133\n",
      "Epoch 43/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.1383 - val_loss: 0.6148\n",
      "Epoch 44/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.1346 - val_loss: 0.6222\n",
      "Epoch 45/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.1325 - val_loss: 0.6249\n",
      "Epoch 46/50\n",
      "16000/16000 [==============================] - 135s 8ms/step - loss: 0.1294 - val_loss: 0.6359\n",
      "Epoch 47/50\n",
      "16000/16000 [==============================] - 143s 9ms/step - loss: 0.1350 - val_loss: 0.6250\n",
      "Epoch 48/50\n",
      "16000/16000 [==============================] - 145s 9ms/step - loss: 0.1539 - val_loss: 0.6544\n",
      "Epoch 49/50\n",
      "16000/16000 [==============================] - 148s 9ms/step - loss: 0.1976 - val_loss: 0.6033\n",
      "Epoch 50/50\n",
      "16000/16000 [==============================] - 147s 9ms/step - loss: 0.2399 - val_loss: 0.6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeid/.virtualenvs/py3env/lib/python3.5/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm_it was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder/concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'encoder/concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model_it.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "model_it.fit([encoder_input_data_it, decoder_input_data_it],  # training data\n",
    "          decoder_target_data_it,                       # labels (left shift of the target sequences)\n",
    "          batch_size=64, epochs=50, validation_split=0.2)\n",
    "\n",
    "model_it.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "### 4.1. Translate English to Italian\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index_it.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index_it.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1, num_decoder_tokens_it))\n",
    "    target_seq[0, 0, target_token_index_it['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model_it.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = numpy.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        if sampled_token_index == 0:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length_it):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1, num_decoder_tokens_it))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:         we waited\n",
      "Italian (true):  abbiamo aspettato\n",
      "Italian (pred):  abbiamo aspettato\n",
      "BLEU score:  1.0\n",
      "-\n",
      "English:         we waited\n",
      "Italian (true):  noi abbiamo aspettato\n",
      "Italian (pred):  abbiamo aspettato\n",
      "BLEU score:  1.0\n",
      "-\n",
      "English:         we waited\n",
      "Italian (true):  aspettammo\n",
      "Italian (pred):  abbiamo aspettato\n",
      "BLEU score:  1.0\n",
      "-\n",
      "English:         we waited\n",
      "Italian (true):  noi aspettammo\n",
      "Italian (pred):  abbiamo aspettato\n",
      "BLEU score:  1.0\n",
      "-\n",
      "English:         we walked\n",
      "Italian (true):  abbiamo camminato\n",
      "Italian (pred):  abbiamo lavaro le pare\n",
      "BLEU score:  0.5454545454545454\n",
      "-\n",
      "English:         we walked\n",
      "Italian (true):  camminavamo\n",
      "Italian (pred):  abbiamo lavaro le pare\n",
      "BLEU score:  0.5454545454545454\n",
      "-\n",
      "English:         we walked\n",
      "Italian (true):  camminammo\n",
      "Italian (pred):  abbiamo lavaro le pare\n",
      "BLEU score:  0.5454545454545454\n",
      "-\n",
      "English:         we yawned\n",
      "Italian (true):  abbiamo sbadigliato\n",
      "Italian (pred):  lo sbbgglliamo\n",
      "BLEU score:  0.7857142857142857\n",
      "-\n",
      "English:         we yawned\n",
      "Italian (true):  sbadigliammo\n",
      "Italian (pred):  lo sbbgglliamo\n",
      "BLEU score:  0.7857142857142857\n",
      "-\n",
      "English:         well try\n",
      "Italian (true):  proveremo\n",
      "Italian (pred):  noi lo tereremo\n",
      "BLEU score:  0.7333333333333333\n",
      "-\n",
      "English:         well try\n",
      "Italian (true):  noi proveremo\n",
      "Italian (pred):  noi lo tereremo\n",
      "BLEU score:  0.7333333333333333\n",
      "-\n",
      "English:         well win\n",
      "Italian (true):  noi vinceremo\n",
      "Italian (pred):  noi canticheremo\n",
      "BLEU score:  0.75\n",
      "-\n",
      "English:         were hot\n",
      "Italian (true):  abbiamo caldo\n",
      "Italian (pred):  siamo checchieve\n",
      "BLEU score:  0.375\n",
      "-\n",
      "English:         were men\n",
      "Italian (true):  siamo uomini\n",
      "Italian (pred):  noi siamo infelici\n",
      "BLEU score:  0.7222222222222222\n",
      "-\n",
      "English:         were men\n",
      "Italian (true):  noi siamo uomini\n",
      "Italian (pred):  noi siamo infelici\n",
      "BLEU score:  0.7222222222222222\n",
      "-\n",
      "English:         were sad\n",
      "Italian (true):  siamo tristi\n",
      "Italian (pred):  siamo affamate\n",
      "BLEU score:  0.5\n",
      "-\n",
      "English:         were sad\n",
      "Italian (true):  noi siamo tristi\n",
      "Italian (pred):  siamo affamate\n",
      "BLEU score:  0.5\n",
      "-\n",
      "English:         were shy\n",
      "Italian (true):  siamo timidi\n",
      "Italian (pred):  siamo bloccate\n",
      "BLEU score:  0.6428571428571429\n",
      "-\n",
      "English:         were shy\n",
      "Italian (true):  noi siamo timidi\n",
      "Italian (pred):  siamo bloccate\n",
      "BLEU score:  0.6428571428571429\n",
      "-\n",
      "English:         were shy\n",
      "Italian (true):  siamo timide\n",
      "Italian (pred):  siamo bloccate\n",
      "BLEU score:  0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "n = 2100, 2120\n",
    "for seq_index in range(n[0], n[1]):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data_it[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    reference = [y[1] for y in clean_pairs_it if y[0] == input_texts_it[seq_index] ]\n",
    "    candidate = decoded_sentence[0:-1]\n",
    "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    \n",
    "    print('-')\n",
    "    print('English:        ', input_texts_it[seq_index])\n",
    "    print('Italian (true): ', target_texts_it[seq_index][1:-1])\n",
    "    print('Italian (pred): ', decoded_sentence[0:-1])\n",
    "    print('BLEU score: ', score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: why is that\n",
      "translated sentence is: lo sono una pioro\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'why is that'\n",
    "\n",
    "input_sequence, input_token_index = text2sequences(max_encoder_seq_length, input_sentence)\n",
    "\n",
    "encoder_input_instance = onehot_encode(input_sequence, max_encoder_seq_length, num_encoder_tokens)\n",
    "\n",
    "translated_sentence = decode_sequence(encoder_input_instance)\n",
    "\n",
    "print('source sentence is: ' + input_sentence)\n",
    "print('translated sentence is: ' + translated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for training set:  0.7422382090580981\n"
     ]
    }
   ],
   "source": [
    "# 1-gram individual BLEU\n",
    "import datetime\n",
    "\n",
    "All_scores = []\n",
    "seq_index = 0\n",
    "verbose = np.inf\n",
    "while seq_index in range(int(0.8*n_train)):\n",
    "    input_seq = encoder_input_data_it[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    reference = [y[1] for y in clean_pairs_it if y[0] == input_texts_it[seq_index] ]\n",
    "    candidate = decoded_sentence[0:-1]\n",
    "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    All_scores += [score]\n",
    "    \n",
    "#     print (seq_index)\n",
    "    if seq_index > verbose:\n",
    "        t = datetime.datetime.now()\n",
    "        print ('{}: sequesnce {} is processed.'.format(t.time(), seq_index))\n",
    "        verbose = seq_index + 1000\n",
    "    seq_index += len(reference)\n",
    "\n",
    "print (\"Average BLEU score for training set: \", np.mean(All_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for validation set:  0.6343801973784912\n"
     ]
    }
   ],
   "source": [
    "All_scores = []\n",
    "seq_index = int(0.8*n_train)+1\n",
    "verbose = np.inf\n",
    "while seq_index in range(int(0.8*n_train)+1, n_train):\n",
    "    input_seq = encoder_input_data_it[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    reference = [y[1] for y in clean_pairs_it if y[0] == input_texts_it[seq_index] ]\n",
    "    candidate = decoded_sentence[0:-1]\n",
    "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    All_scores += [score]\n",
    "    \n",
    "    if seq_index > verbose:\n",
    "        t = datetime.datetime.now()\n",
    "        print ('{}: sequesnce {} is processed.'.format(t.time(), seq_index))\n",
    "        verbose = seq_index + 1000\n",
    "    seq_index += len(reference)\n",
    "\n",
    "print (\"Average BLEU score for validation set: \", np.mean(All_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
